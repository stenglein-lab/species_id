params {

  // -------------------------
  // Input/Output directories
  // -------------------------
  input_dir    = "$baseDir/input/"
  fastq_dir    = "${params.input_dir}/fastq/"
  outdir       = "$baseDir/results/"
  refseq_dir   = "$baseDir/refseq/"

  // create hard links of output files in results folder
  publish_dir_mode          = "link"

  // pattern to match for fastq files
  fastq_pattern             = "*_R[12]*.fastq*"

  fastq_out_dir         = "${params.outdir}/trimmed_fastq/"
  bam_out_dir           = "${params.outdir}/bam/"

  // reports on running the pipeline itself
  tracedir = "${outdir}/pipeline_info"

  // where are R and shell scripts are found.
  bin_dir  = "${baseDir}/bin"

  // fasta file with set of reference sequences for doing species I
  species_id_fasta       = "${params.refseq_dir}/co1.fasta"

  // miniminum mapping quality for a co1-mapping read to be considered 
  // for host species assignment
  min_species_id_mapq = 40

  // maximum distance paired reads are allowed to map apart (passed as an option to bowtie2
  max_paired_insert_length = 600

  //
  // turn this parameter on to pull docker containers and convert to singularity
  //
  // see e.g.: https://nf-co.re/gwas#quick-start, which states:
  //
  //   "If you are persistently observing issues downloading Singularity images directly
  //    due to timeout or network issues then please use the --singularity_pull_docker_container
  //    parameter to pull and convert the Docker image instead."
  //
  // TODO: this option is provided in nf-core pipelines but is it necessary?
  //       possibly remove this option and the corresponding if/else statment in processes?
  //

  singularity_pull_docker_container = false

  // Max resource options
  // Defaults only, expecting to be overwritten
  max_memory                 = '384.GB'
  max_cpus                   = 64
  max_time                   = '240.h'

}

// Load modules.config for DSL2 module specific options
includeConfig 'conf/modules.config'

// resource limits defined here
includeConfig 'conf/base.config'

process {

  // ------------------------------------------------------------
  // setup resource usage limits for different types of processes
  // ------------------------------------------------------------

  // high memory process like blastn (using nt database)
  withLabel: 'highmem' {
    maxForks = 2
    cpus = 24
  }

  // low memory processes that use multi-threading
  // like bowtie2
  withLabel: 'lowmem_threaded' {
    maxForks = 8
    cpus = 8
  }

  // low memory processes that don't use multi-threading
  withLabel: 'lowmem_non_threaded' {
    maxForks = 24
    cpus = 1
  }
}

/*
   Profiles allow you to run on different servers or with different base configurations

   See: https://www.nextflow.io/docs/latest/config.html#config-profiles
*/
profiles {

  local {
    exector.name = 'local'
    // if the pipeline has to access system paths outside of $HOME, $PWD, etc 
    // have to bind those paths to singularity.
    // see: https://sylabs.io/guides/latest/user-guide/bind_paths_and_mounts.html
    // in this profile, we are pointing to local intallations of NCBI databases 
    //so need to access those paths
    // singularity.runOptions = "--bind /home/databases"
    // params.remote_blast = false
  }

  conda {
    params.enable_conda    = true
    singularity.enabled    = false
    conda.cacheDir         = "$HOME/conda_cacheDir"
  }

  singularity {
    params.enable_conda    = false
    singularity.enabled    = true
    singularity.autoMounts = true
    singularity.cacheDir   = "$HOME/singularity_cacheDir"
  }

  test {
    includeConfig 'conf/test.config'
  }
}

def trace_timestamp = new java.util.Date().format( 'yyyy-MM-dd_HH-mm-ss')
timeline {
    enabled = true
    file    = "${params.tracedir}/execution_timeline_${trace_timestamp}.html"
}
report {
    enabled = true
    file    = "${params.tracedir}/execution_report_${trace_timestamp}.html"
}
trace {
    enabled = true
    file    = "${params.tracedir}/execution_trace_${trace_timestamp}.txt"
}
dag {
    enabled = true
    file    = "${params.tracedir}/pipeline_dag_${trace_timestamp}.pdf"
}

manifest {
    name            = 'stenglein-lab/species_id'
    author          = 'Mark Stenglein'
    homePage        = 'https://github.com/stenglein-lab/species_id'
    description     = 'A pipeline to assign species based on reads mapping to a set of discriminating reference sequences (e.g. CO1 sequences) '
    mainScript      = 'main.nf'
    nextflowVersion = '!>=21.04.0'
    version         = '1.0'
}

// Function to ensure that resource requirements don't go beyond
// a maximum limit
// From nf-core pipelines
def check_max(obj, type) {
    if (type == 'memory') {
        try {
            if (obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)
                return params.max_memory as nextflow.util.MemoryUnit
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max memory '${params.max_memory}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'time') {
        try {
            if (obj.compareTo(params.max_time as nextflow.util.Duration) == 1)
                return params.max_time as nextflow.util.Duration
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max time '${params.max_time}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'cpus') {
        try {
            return Math.min( obj, params.max_cpus as int )
        } catch (all) {
            println "   ### ERROR ###   Max cpus '${params.max_cpus}' is not valid! Using default value: $obj"
            return obj
        }
    }
}


// Turn this option on to delete all intermediate files from the analysis
// see: https://www.nextflow.io/docs/latest/config.html
// cleanup = true

